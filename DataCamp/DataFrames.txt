Spark's core data structure is the Resilient Distributed Dataset (RDD). 
This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. 
However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.

The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). 
Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.

To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. 
You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.